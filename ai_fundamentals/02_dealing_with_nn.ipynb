{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Neural Networks\n",
    "Deep learning is a class of machine learning algorithms that is designed to loosely mimic\n",
    "the neurons in our brain. A neuron takes an input from a number of inputs from\n",
    "surrounding neurons and sums it up, and if the sum exceeds a certain threshold, then the\n",
    "neuron fires.\n",
    "The following is a diagram of a neural unit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural_network](../media/nnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Fashionâ€“MNIST dataset. This is a dataset of Zalando's article images,\n",
    "consisting of a training set of 60,000 examples and a test set of 10,000 examples. We will\n",
    "take an individual grayscale image 28 x 28 in size and convert it into a vector of 784."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define transforms for the preprocessing of our image data.\n",
    "For the particular case that we are dealing with, an image consisting of 28 x 28 grayscale pixels, we first need to read from the image and convert it into a tensor using a transforms.ToTensor() transform. We then make the mean and\n",
    "standard deviation of the pixel values 0.5 and 0.5 respectively so that it becomes easier for the model to train; to do this, we use transforms.Normalize((0.5,),(0.5,)). We combine all of the transformations together with transform.Compose()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the batch_size to divide our dataset into chunks to be fed into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull the dataset from torchvision and apply the transform and create batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to Fashion_MNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb5f7cb7de84c778316e7803fea8818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Fashion_MNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to Fashion_MNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to Fashion_MNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b88c8c14e8c464880bc9ff4e7384405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Fashion_MNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to Fashion_MNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to Fashion_MNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f72d1652b64d71b8336cec8a328e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Fashion_MNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to Fashion_MNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to Fashion_MNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1063c1dd59455c96a75b2bec20f26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Fashion_MNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to Fashion_MNIST/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download and load the train data\n",
    "trainset = datasets.FashionMNIST('Fashion_MNIST/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('Fashion_MNIST/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural network class, which has to be a subclass of nn.Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        output = self.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use nn.Linear() to define fully connected layers by passing in the input and output dimensions.<br>\n",
    "We use a softmax layer for the last layer output because there are 10 output classes.<br>\n",
    "We use ReLU activation in the layers before the output layer to learn nonlinearity in the data.<br>\n",
    "The hidden1 layer takes 784 inputs units and gives out 256 output units.<br>\n",
    "The hidden2 phrase outputs 128 units and the output layer has 10 output units representing 10 output classes.<br>\n",
    "The softmax layer converts the activations into probabilities so that it adds to 1 along dimension 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another method that we can use to define models using nn.Sequential() and pass in the required layers without needing to define a class. There are also other transformations that can be applied to the input image that we will explore in the subsequent chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then create the network object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionNetwork(\n",
      "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recipe, the network is completed by setting up a forward network, wherein we tied together the network components defined in the constructor. A network defined with nn.Module needs to have a forward() method defined. It takes the input tensor and passes it through the network components defined in the __init__() method in the network class, in the sequence of operations defined in the forward method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, we will define a loss function for our fashion dataset using the loss function\n",
    "available in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, we replaced softmax with log softmax so that we could then use the log of\n",
    "probabilities over probabilities, which has nice theoretic interpretations. There are various\n",
    "reasons for doing this, including improved numerical performance and gradient\n",
    "optimization. These advantages can be extremely important when training a model that can\n",
    "be computationally challenging and expensive. Furthermore, it has a high penalizing effect\n",
    "when it is not predicting the correct class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.activation = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.output(x)\n",
    "        output = self.log_softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionNetwork(\n",
      "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=1)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FashionNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define our loss function; we will use negative log likelihood loss\n",
    "for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a method by which the neural networks learn from errors; the errors are used to modify weights in such a way that the errors are minimized. Optimization functions are responsible for modifying\n",
    "weights to reduce the error. Optimization functions calculate the partial derivative of errors with respect to weights. The derivative shows the direction of a positive slope, and so we need to reverse the direction of the gradient. The optimizer function combines the model parameters and loss function to iteratively modify the model parameters to reduce the model error. Optimizers can be thought of as fiddling with the model weights to get the\n",
    "best possible model based on the difference in prediction from the model and the actual output, and the loss function acts as a guide by indicating when the optimizer is going right or wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Adam optimizer and pass model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.001,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False,\n",
       " 'maximize': False,\n",
       " 'foreach': None,\n",
       " 'capturable': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.003\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start training our model, starting with the number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4926\n",
      "Training loss: 0.3888\n",
      "Training loss: 0.3509\n",
      "Training loss: 0.3291\n",
      "Training loss: 0.3147\n",
      "Training loss: 0.3026\n",
      "Training loss: 0.2906\n",
      "Training loss: 0.2776\n",
      "Training loss: 0.2733\n",
      "Training loss: 0.2650\n"
     ]
    }
   ],
   "source": [
    "for _ in range(epoch):\n",
    "    running_loss = 0 #initialize running_loss\n",
    "    for image, label in trainloader: # we will iterate through each image in training the image loader\n",
    "        optimizer.zero_grad() # reset the gradients to zero\n",
    "        image = image.view(image.shape[0],-1) # reshape the image\n",
    "        pred = model(image) # get the prediction\n",
    "        loss = criterion(pred, label) # calculate the loss/error\n",
    "        loss.backward() # we call the .backward() method on the loss\n",
    "        optimizer.step() # we call the .step() method on the optimizer\n",
    "        running_loss += loss.item() # we append to the running loss\n",
    "    else:\n",
    "        print(f'Training loss: {running_loss/len(trainloader):.4f}') # print the loss after each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing dropouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting happens when a model learns the data that is given to it for training rather than generalizing on the solution spaceâ€”that is, it learns the minute details and noises of the training data, instead of grasping the bigger\n",
    "picture, and so performs poorly on new data. Regularization is the process of preventing models from overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a dropout is one of the most popular regularization techniques in neural networks, in which randomly selected neurons are turned off while trainingâ€”that is, the contribution of neurons is temporarily removed from the forward pass and the backward pass doesn't affect the weights, so that no single neuron or subset of neurons gets all the decisive power of the model; rather, all the neurons are forced to make active contributions to predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropouts can be intuitively understood as creating a large number of ensemble models, learning to capture various features under one big definition of a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.25) # add dropout layer with a 0.25 prob, 25% of neurones will be turned off\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop(x) # dropouts must be applied on hidden layers to prevent losing input/output data\n",
    "        x = self.hidden2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.output(x)\n",
    "        output = self.log_softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionNetwork(\n",
      "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (log_softmax): LogSoftmax(dim=None)\n",
      "  (activation): ReLU()\n",
      "  (drop): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FashionNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing functional APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784,256)\n",
    "        self.hidden2 = nn.Linear(256,128)\n",
    "        self.output = nn.Linear(128,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.log_softmax(self.output(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7102b94f461f617a8f2699728606b38c4f3ffd674d9bf8fdf419e43172049633"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
