{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will deal with recurrent neural networks (RNNs), a kind of neural network that specializes in dealing with sequential or time-varying data. <br>\n",
    "The data points have a temporal relationship to one another. In a recurrent neural network, connections between neurons form a directed graph on a temporal sequence, exhibiting temporal dynamic behavior. <br>\n",
    "A traditional feed-forward network has no memory of the previous input; however, an RNN uses a memory unit to remember the previous input and therefore processes the current input based on the sequence of inputs so far. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where information from one step is being fed into the next, creating multiple copies of the same network, and\n",
    "all this is encapsulated in the recurrent loop. A recurrent neural network accepts an input and gives an output, but this output is dependent not just on the input at the given instance, but on the entire history of inputs given to the network, which are mathematically remembered by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torchtext) (1.21.5)\n",
      "Requirement already satisfied: torch==1.8.0 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torchtext) (1.8.0)\n",
      "Requirement already satisfied: tqdm in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torchtext) (4.64.0)\n",
      "Requirement already satisfied: requests in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torchtext) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torch==1.8.0->torchtext) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with a natural language processing task, we take a text corpus and break it down into smaller units. <br>\n",
    "A computer can only understand numbers, and so these words are assigned a unique integer value to represent a word. The process of breaking a sentence into tokens is called tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda words : words.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'for', 'tokenizer']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"This is a test for tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields make it easy to process natural-language data. Fields let us define the datatype and help us create\n",
    "tensors out of textual data by specifying the set of operations to be performed on the data.\n",
    "The Field class lets us perform common text processing tasks and holds the vocabulary of the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.9.0\n",
      "  Downloading torchtext-0.9.0-cp39-cp39-macosx_10_9_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.8.0\n",
      "  Downloading torch-1.8.0-cp39-none-macosx_10_9_x86_64.whl (120.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 120.6 MB 2.2 kB/s  eta 0:00:01    |█████████▋                      | 36.1 MB 11.6 MB/s eta 0:00:08     |███████████████▍                | 58.1 MB 7.9 MB/s eta 0:00:08     |███████████████▋                | 59.0 MB 7.9 MB/s eta 0:00:08     |██████████████████████          | 82.6 MB 6.8 MB/s eta 0:00:06███████████████       | 94.2 MB 24.7 MB/s eta 0:00:02| 107.9 MB 29.9 MB/s eta 0:00:01     |██████████████████████████████▋ | 115.3 MB 25.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.9.0) (1.21.5)\n",
      "Requirement already satisfied: requests in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.9.0) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torchtext==0.9.0) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from torch==1.8.0->torchtext==0.9.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/maxi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchtext==0.9.0) (2022.6.15)\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.13.1\n",
      "    Uninstalling torchtext-0.13.1:\n",
      "      Successfully uninstalled torchtext-0.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0 requires torch==1.12.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.8.0 torchtext-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torchtext==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Field object for reviews and the field for labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review = Field(sequential=True, tokenize=tokenizer, lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can: <br>\n",
    "- add a token at the beginning and end of an input string.<br>\n",
    "- set the sequence to a fixed length.<br>\n",
    "- set an unknown token.<br>\n",
    "- set the batch dimension as the first dimension.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SequenceField = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "SequenceField = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', lower=True, fix_length=50)\n",
    "SequenceField = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', unk_token='<unk>')\n",
    "SequenceField = Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>', unk_token='<unk>', batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import TabularDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the training columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datafields = [(\"id\", None),\n",
    "                    (\"content\", Review), (\"Business\", Label),\n",
    "                    (\"SciTech\", Label), (\"Sports\", Label),\n",
    "                    (\"World\", Label)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the testing columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datafields = [(\"id\", None),\n",
    "                    (\"content\", Review)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the training and validation .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TabularDataset(path='NewsClassification/train.csv', \n",
    "                        format='csv',\n",
    "                        skip_header=True,\n",
    "                        fields=train_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = TabularDataset(path='NewsClassification/valid.csv', \n",
    "                        format='csv',\n",
    "                        skip_header=True,\n",
    "                        fields=train_datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TabularDataset(path=\"NewsClassification/test.csv\",\n",
    "                    format='csv',\n",
    "                    skip_header=True,\n",
    "                    fields=test_datafields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing iterators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterators are used to load batches of data from the dataset. They provide methods to make loading data and moving data to the appropriate device easier. We could use these iterator objects to iterate over the data while running through the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import BucketIterator\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the device that's available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use BucketIterator to create buckets of datasets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "                                    (train, valid, test),\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    device=device,\n",
    "                                    sort_key=lambda x: len(x.comment_text), \n",
    "                                    sort_within_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are learned representations of words. They are dense representations of words, where each word is assigned a vector, that is, a real-valued vector in a pre-defined vector space, rather than a numerical identifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In word embedding, words with similar meanings have a similar representation, and we can perform vector arithmetic on these word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move on to loading the embedding vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./vec/glove_embedding/glove.6B.zip: 862MB [02:42, 5.29MB/s]                                \n",
      "100%|█████████▉| 399999/400000 [00:24<00:00, 16255.47it/s]\n"
     ]
    }
   ],
   "source": [
    "vec = vocab.Vectors('glove.6B.100d.txt',\n",
    "    cache='./vec/glove_embedding/',\n",
    "    url='http://nlp.stanford.edu/data/glove.6B.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build the vocabulary from the pretrained vector by applying it to the field object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review.build_vocab(train, min_freq=2, vectors=vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded the pretrained word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an LSTM network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM) networks are a type of recurrent neural network that has internal gates that helps in better information persistence. These gates are tiny neural networks that control when information needs to be saved and when it can be erased or forgotten.<br>\n",
    "RNNs suffer from vanishing and exploding gradients, making it difficult to learn long-term dependencies. LSTMs are resistant to exploding and vanishing gradients, although it is still mathematically possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name the class LSTMClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "        \"\"\"\"Constructor of the class\"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(Review.vocab), embedding_dim) # add the embedding layer\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim) # add the LSTM layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # add a fully connected layer\n",
    "        self.dropout = nn.Dropout(dropout) # define the dropout layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, num_layers):\n",
    "        \"\"\"\"Constructor of the class\"\"\"\n",
    "        super(MultiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(Review.vocab), embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT = 1\n",
    "DROPOUT = 0.5\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a normal LSTM, the LSTM reads the input sequence from first to last; however, in a bidirectional LSTM, there is a second LSTM that reads the sequence from last to first—that is, a backward RNN.<br>\n",
    "This type of LSTM improves the model performance when the prediction at the current timestamp is dependent on the inputs further on in the sequence. Consider the examples \"I read comics\" and \"I read comics yesterday\". In this case, the same token, that is, read, has different meanings based on the token that appears in the future. We will explore its implementation in this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, num_layers):\n",
    "        \"\"\"\"Constructor of the class\"\"\"\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(Review.vocab), embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenated the hidden states of the forward and backward LSTMs and passed them into the fully connected layer. Because of this, the input dimension of the fully connected layer was doubled to accommodate the forward and backward hidden state tensors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7102b94f461f617a8f2699728606b38c4f3ffd674d9bf8fdf419e43172049633"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
